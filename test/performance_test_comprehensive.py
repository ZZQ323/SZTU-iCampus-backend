#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SZTU-iCampus å…¨é¢æ€§èƒ½æµ‹è¯•è„šæœ¬
é¢å‘30åˆ†é’Ÿåä¸Šçº¿ç­”è¾©çš„æ€§èƒ½éªŒè¯

æµ‹è¯•æŒ‡æ ‡ï¼š
1. è¯·æ±‚è€—æ—¶ï¼ˆç«¯åˆ°ç«¯å“åº”æ—¶é—´ï¼‰
2. æ•°æ®åº“æŸ¥è¯¢è€—æ—¶ï¼ˆçº¯SQLæ‰§è¡Œæ—¶é—´ï¼‰
3. å‹åŠ›æµ‹è¯•ï¼ˆå¹¶å‘å¤„ç†èƒ½åŠ›ï¼‰

ä½œè€…ï¼šClaude Sonnet 4
æ—¥æœŸï¼š2025-06-25
"""

import asyncio
import time
import json
import httpx
import sqlite3
import psutil
import threading
from datetime import datetime
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from concurrent.futures import ThreadPoolExecutor
import logging
import statistics

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('performance_test.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡æ•°æ®ç±»"""
    test_name: str
    total_time: float
    db_query_time: float = 0.0
    network_time: float = 0.0
    cache_hit: bool = False
    status_code: int = 0
    data_size: int = 0
    error_message: str = ""
    timestamp: str = ""

@dataclass
class StressTestResult:
    """å‹åŠ›æµ‹è¯•ç»“æœæ•°æ®ç±»"""
    concurrent_users: int
    total_requests: int
    successful_requests: int
    failed_requests: int
    avg_response_time: float
    min_response_time: float
    max_response_time: float
    qps: float
    cpu_usage: float
    memory_usage: float

class DatabaseQueryTester:
    """æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½æµ‹è¯•å™¨"""
    
    def __init__(self, db_path="data-service/sztu_campus.db"):
        self.db_path = db_path
        self.headers = {"X-API-Key": "sztu-data-service-key-2024"}
        self.data_service_url = "http://localhost:8001"
    
    async def test_direct_sql_performance(self) -> Dict[str, float]:
        """ç›´æ¥æµ‹è¯•SQLæ‰§è¡Œæ€§èƒ½"""
        logger.info("ğŸ” å¼€å§‹æ•°æ®åº“ç›´æ¥æŸ¥è¯¢æ€§èƒ½æµ‹è¯•...")
        
        test_cases = {
            "ç®€å•é€‰æ‹©æŸ¥è¯¢": "SELECT * FROM persons WHERE person_type='student' LIMIT 10",
            "å¤æ‚JOINæŸ¥è¯¢": """
                SELECT p.name, c.college_name, m.major_name 
                FROM persons p 
                JOIN colleges c ON p.college_id = c.college_id 
                JOIN majors m ON p.major_id = m.major_id 
                WHERE p.person_type='student' LIMIT 10
            """,
            "èšåˆç»Ÿè®¡æŸ¥è¯¢": """
                SELECT college_id, COUNT(*) as student_count 
                FROM persons 
                WHERE person_type='student' AND is_deleted=0 
                GROUP BY college_id
            """,
            "å¤§è¡¨æŸ¥è¯¢": """
                SELECT e.*, ci.course_name 
                FROM enrollments e 
                JOIN course_instances ci ON e.course_instance_id = ci.instance_id 
                WHERE e.enrollment_status='completed' LIMIT 50
            """
        }
        
        results = {}
        
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            for test_name, sql in test_cases.items():
                times = []
                for i in range(3):  # æ¯ä¸ªæŸ¥è¯¢æ‰§è¡Œ3æ¬¡å–å¹³å‡å€¼
                    start_time = time.time()
                    cursor.execute(sql)
                    rows = cursor.fetchall()
                    end_time = time.time()
                    
                    query_time = (end_time - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
                    times.append(query_time)
                    
                    logger.info(f"  {test_name} ç¬¬{i+1}æ¬¡: {query_time:.2f}ms, è¿”å›{len(rows)}è¡Œ")
                
                avg_time = statistics.mean(times)
                results[test_name] = avg_time
                logger.info(f"âœ… {test_name} å¹³å‡è€—æ—¶: {avg_time:.2f}ms")
            
            conn.close()
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“æŸ¥è¯¢æµ‹è¯•å¤±è´¥: {e}")
            
        return results
    
    async def test_api_vs_direct_query(self) -> Dict[str, Dict[str, float]]:
        """å¯¹æ¯”APIè°ƒç”¨å’Œç›´æ¥æŸ¥è¯¢çš„æ€§èƒ½å·®å¼‚"""
        logger.info("ğŸ“Š å¼€å§‹API vs ç›´æ¥æŸ¥è¯¢æ€§èƒ½å¯¹æ¯”...")
        
        # æµ‹è¯•æ¡ˆä¾‹ï¼šè·å–å­¦ç”ŸåŸºæœ¬ä¿¡æ¯
        student_id = "202100043213"
        
        # 1. ç›´æ¥æ•°æ®åº“æŸ¥è¯¢
        start_time = time.time()
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            cursor.execute("""
                SELECT p.*, c.college_name, m.major_name 
                FROM persons p 
                LEFT JOIN colleges c ON p.college_id = c.college_id 
                LEFT JOIN majors m ON p.major_id = m.major_id 
                WHERE p.student_id = ?
            """, (student_id,))
            direct_result = cursor.fetchone()
            conn.close()
            direct_time = (time.time() - start_time) * 1000
            
        except Exception as e:
            logger.error(f"ç›´æ¥æŸ¥è¯¢å¤±è´¥: {e}")
            direct_time = 0
        
        # 2. data-service APIæŸ¥è¯¢
        start_time = time.time()
        try:
            async with httpx.AsyncClient(timeout=10.0) as client:
                response = await client.get(
                    f"{self.data_service_url}/query/persons",
                    params={
                        "filters": json.dumps({"student_id": student_id}),
                        "join_tables": "colleges,majors",
                        "limit": 1
                    },
                    headers=self.headers
                )
                api_time = (time.time() - start_time) * 1000
                api_success = response.status_code == 200
                
        except Exception as e:
            logger.error(f"APIæŸ¥è¯¢å¤±è´¥: {e}")
            api_time = 0
            api_success = False
        
        results = {
            "ç›´æ¥æ•°æ®åº“æŸ¥è¯¢": {"è€—æ—¶(ms)": direct_time, "æˆåŠŸ": True},
            "data-service API": {"è€—æ—¶(ms)": api_time, "æˆåŠŸ": api_success}
        }
        
        if direct_time > 0 and api_time > 0:
            overhead = api_time - direct_time
            overhead_percent = (overhead / direct_time) * 100
            results["APIå¼€é”€åˆ†æ"] = {
                "é¢å¤–è€—æ—¶(ms)": overhead,
                "å¼€é”€ç™¾åˆ†æ¯”(%)": overhead_percent
            }
            logger.info(f"ğŸ“ˆ APIè°ƒç”¨å¼€é”€: {overhead:.2f}ms ({overhead_percent:.1f}%)")
        
        return results

class EndToEndTester:
    """ç«¯åˆ°ç«¯æ€§èƒ½æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.backend_url = "http://127.0.0.1:8000"
        self.test_user = {"login_id": "202100043213", "password": "123456"}
        self.auth_token = None
        self.metrics: List[PerformanceMetrics] = []
    
    async def authenticate(self) -> bool:
        """ç”¨æˆ·è®¤è¯"""
        try:
            start_time = time.time()
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(
                    f"{self.backend_url}/api/v1/auth/login",
                    json=self.test_user
                )
                total_time = (time.time() - start_time) * 1000
                
                if response.status_code == 200:
                    data = response.json()
                    self.auth_token = data.get("data", {}).get("access_token")
                    
                    # è®°å½•ç™»å½•æ€§èƒ½
                    metric = PerformanceMetrics(
                        test_name="ç”¨æˆ·ç™»å½•",
                        total_time=total_time,
                        status_code=response.status_code,
                        data_size=len(response.content),
                        timestamp=datetime.now().isoformat()
                    )
                    self.metrics.append(metric)
                    
                    logger.info(f"âœ… ç™»å½•æˆåŠŸ: {total_time:.2f}ms")
                    return True
                else:
                    logger.error(f"âŒ ç™»å½•å¤±è´¥: {response.status_code}")
                    return False
                    
        except Exception as e:
            logger.error(f"âŒ ç™»å½•å¼‚å¸¸: {e}")
            return False
    
    async def test_core_apis(self) -> List[PerformanceMetrics]:
        """æµ‹è¯•æ ¸å¿ƒAPIæ€§èƒ½"""
        if not self.auth_token:
            await self.authenticate()
        
        headers = {"Authorization": f"Bearer {self.auth_token}"}
        
        # æ ¸å¿ƒAPIæµ‹è¯•ç”¨ä¾‹
        test_cases = [
            {
                "name": "è¯¾è¡¨æŸ¥è¯¢",
                "url": f"{self.backend_url}/api/v1/schedule/",
                "params": {"semester": "2024-2025-1"},
                "critical": True  # å…³é”®ä¸šåŠ¡
            },
            {
                "name": "æˆç»©æŸ¥è¯¢", 
                "url": f"{self.backend_url}/api/v1/grades",
                "params": {"semester": "2024-2025-1"},
                "critical": True
            },
            {
                "name": "å…¬å‘Šåˆ—è¡¨",
                "url": f"{self.backend_url}/api/v1/announcements",
                "params": {"page": 1, "size": 10},
                "critical": False
            },
            {
                "name": "è€ƒè¯•åˆ—è¡¨",
                "url": f"{self.backend_url}/api/v1/exams",
                "params": {"limit": 10},
                "critical": False
            }
        ]
        
        logger.info("ğŸš€ å¼€å§‹æ ¸å¿ƒAPIæ€§èƒ½æµ‹è¯•...")
        
        for test_case in test_cases:
            # æ¯ä¸ªAPIæµ‹è¯•3æ¬¡å–å¹³å‡å€¼
            times = []
            success_count = 0
            
            for i in range(3):
                start_time = time.time()
                try:
                    async with httpx.AsyncClient(timeout=30.0) as client:
                        response = await client.get(
                            test_case["url"],
                            params=test_case["params"],
                            headers=headers
                        )
                        total_time = (time.time() - start_time) * 1000
                        times.append(total_time)
                        
                        if response.status_code == 200:
                            success_count += 1
                            
                        # ç¬¬ä¸€æ¬¡è¯·æ±‚è®°å½•è¯¦ç»†ä¿¡æ¯
                        if i == 0:
                            metric = PerformanceMetrics(
                                test_name=test_case["name"],
                                total_time=total_time,
                                status_code=response.status_code,
                                data_size=len(response.content),
                                timestamp=datetime.now().isoformat()
                            )
                            self.metrics.append(metric)
                        
                        logger.info(f"  {test_case['name']} ç¬¬{i+1}æ¬¡: {total_time:.2f}ms")
                        
                except Exception as e:
                    logger.error(f"  {test_case['name']} ç¬¬{i+1}æ¬¡å¤±è´¥: {e}")
                    times.append(0)
            
            # è®¡ç®—å¹³å‡æ€§èƒ½
            valid_times = [t for t in times if t > 0]
            if valid_times:
                avg_time = statistics.mean(valid_times)
                success_rate = success_count / 3 * 100
                
                # æ€§èƒ½è¯„ä¼°
                performance_level = self._evaluate_performance(avg_time, test_case["critical"])
                
                logger.info(f"ğŸ“Š {test_case['name']} æ±‡æ€»:")
                logger.info(f"   å¹³å‡å“åº”æ—¶é—´: {avg_time:.2f}ms")
                logger.info(f"   æˆåŠŸç‡: {success_rate:.1f}%")
                logger.info(f"   æ€§èƒ½ç­‰çº§: {performance_level}")
                
        return self.metrics
    
    async def test_cache_effectiveness(self) -> Dict[str, Any]:
        """æµ‹è¯•ç¼“å­˜æ•ˆæœ"""
        logger.info("ğŸ”„ æµ‹è¯•ç¼“å­˜æ•ˆæœ...")
        
        if not self.auth_token:
            await self.authenticate()
        
        headers = {"Authorization": f"Bearer {self.auth_token}"}
        
        # æ¸…ç©ºç¼“å­˜ï¼ˆå¦‚æœæœ‰ç›¸å…³APIï¼‰
        try:
            async with httpx.AsyncClient() as client:
                await client.delete(f"{self.backend_url}/api/v1/cache/clear", headers=headers)
        except:
            pass  # å¦‚æœæ²¡æœ‰æ¸…ç¼“å­˜APIä¹Ÿæ²¡å…³ç³»
        
        # ç¬¬ä¸€æ¬¡è¯·æ±‚ï¼ˆç¼“å­˜æœªå‘½ä¸­ï¼‰
        start_time = time.time()
        async with httpx.AsyncClient(timeout=30.0) as client:
            response1 = await client.get(
                f"{self.backend_url}/api/v1/schedule/",
                params={"semester": "2024-2025-1"},
                headers=headers
            )
            cache_miss_time = (time.time() - start_time) * 1000
        
        # ç­‰å¾…1ç§’å†æ¬¡è¯·æ±‚ï¼ˆåº”è¯¥å‘½ä¸­ç¼“å­˜ï¼‰
        await asyncio.sleep(1)
        start_time = time.time()
        async with httpx.AsyncClient(timeout=30.0) as client:
            response2 = await client.get(
                f"{self.backend_url}/api/v1/schedule/",
                params={"semester": "2024-2025-1"},
                headers=headers
            )
            cache_hit_time = (time.time() - start_time) * 1000
        
        # åˆ†æç¼“å­˜æ•ˆæœ
        improvement = (cache_miss_time - cache_hit_time) / cache_miss_time * 100
        cache_effective = improvement > 10  # è¶…è¿‡10%æ”¹å–„è®¤ä¸ºç¼“å­˜æœ‰æ•ˆ
        
        cache_result = {
            "ç¼“å­˜æœªå‘½ä¸­æ—¶é—´(ms)": cache_miss_time,
            "ç¼“å­˜å‘½ä¸­æ—¶é—´(ms)": cache_hit_time,
            "æ€§èƒ½æå‡(%)": improvement,
            "ç¼“å­˜æœ‰æ•ˆ": cache_effective,
            "æå‡å€æ•°": cache_miss_time / cache_hit_time if cache_hit_time > 0 else 0
        }
        
        logger.info(f"ğŸ“ˆ ç¼“å­˜æ•ˆæœåˆ†æ:")
        logger.info(f"   ç¼“å­˜æœªå‘½ä¸­: {cache_miss_time:.2f}ms")
        logger.info(f"   ç¼“å­˜å‘½ä¸­: {cache_hit_time:.2f}ms")
        logger.info(f"   æ€§èƒ½æå‡: {improvement:.1f}%")
        logger.info(f"   ç¼“å­˜çŠ¶æ€: {'âœ… æœ‰æ•ˆ' if cache_effective else 'âŒ æ— æ•ˆ'}")
        
        return cache_result
    
    def _evaluate_performance(self, response_time: float, is_critical: bool) -> str:
        """è¯„ä¼°æ€§èƒ½ç­‰çº§"""
        if is_critical:
            # å…³é”®ä¸šåŠ¡æ›´ä¸¥æ ¼çš„æ ‡å‡†
            if response_time < 500:
                return "ğŸŸ¢ ä¼˜ç§€"
            elif response_time < 1000:
                return "ğŸŸ¡ è‰¯å¥½"
            elif response_time < 2000:
                return "ğŸŸ  ä¸€èˆ¬"
            else:
                return "ğŸ”´ éœ€ä¼˜åŒ–"
        else:
            # éå…³é”®ä¸šåŠ¡æ ‡å‡†
            if response_time < 1000:
                return "ğŸŸ¢ ä¼˜ç§€"
            elif response_time < 2000:
                return "ğŸŸ¡ è‰¯å¥½" 
            elif response_time < 3000:
                return "ğŸŸ  ä¸€èˆ¬"
            else:
                return "ğŸ”´ éœ€ä¼˜åŒ–"

class StressTester:
    """å‹åŠ›æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.backend_url = "http://127.0.0.1:8000"
        self.test_users = [
            {"login_id": "202100043213", "password": "123456"},
            {"login_id": "202100008036", "password": "123456"},
            # å¯ä»¥æ·»åŠ æ›´å¤šæµ‹è¯•ç”¨æˆ·
        ]
        self.results: List[StressTestResult] = []
    
    async def concurrent_login_test(self, concurrent_users: int = 20) -> StressTestResult:
        """å¹¶å‘ç™»å½•æµ‹è¯•"""
        logger.info(f"ğŸ”¥ å¼€å§‹å¹¶å‘ç™»å½•æµ‹è¯• - {concurrent_users}ä¸ªå¹¶å‘ç”¨æˆ·...")
        
        start_time = time.time()
        
        # è®°å½•ç³»ç»Ÿèµ„æºä½¿ç”¨
        initial_cpu = psutil.cpu_percent()
        initial_memory = psutil.virtual_memory().percent
        
        async def single_login_test(user_index: int) -> Dict[str, Any]:
            """å•ä¸ªç”¨æˆ·ç™»å½•æµ‹è¯•"""
            user = self.test_users[user_index % len(self.test_users)]
            
            try:
                request_start = time.time()
                async with httpx.AsyncClient(timeout=30.0) as client:
                    response = await client.post(
                        f"{self.backend_url}/api/v1/auth/login",
                        json=user
                    )
                    request_time = (time.time() - request_start) * 1000
                    
                    return {
                        "success": response.status_code == 200,
                        "response_time": request_time,
                        "status_code": response.status_code
                    }
            except Exception as e:
                return {
                    "success": False,
                    "response_time": 0,
                    "error": str(e)
                }
        
        # å¹¶å‘æ‰§è¡Œç™»å½•æµ‹è¯•
        tasks = [single_login_test(i) for i in range(concurrent_users)]
        test_results = await asyncio.gather(*tasks)
        
        total_time = time.time() - start_time
        
        # è®°å½•ç³»ç»Ÿèµ„æºä½¿ç”¨
        final_cpu = psutil.cpu_percent()
        final_memory = psutil.virtual_memory().percent
        
        # ç»Ÿè®¡ç»“æœ
        successful = [r for r in test_results if r["success"]]
        failed = [r for r in test_results if not r["success"]]
        response_times = [r["response_time"] for r in successful if r["response_time"] > 0]
        
        result = StressTestResult(
            concurrent_users=concurrent_users,
            total_requests=len(test_results),
            successful_requests=len(successful),
            failed_requests=len(failed),
            avg_response_time=statistics.mean(response_times) if response_times else 0,
            min_response_time=min(response_times) if response_times else 0,
            max_response_time=max(response_times) if response_times else 0,
            qps=len(successful) / total_time if total_time > 0 else 0,
            cpu_usage=final_cpu,
            memory_usage=final_memory
        )
        
        self.results.append(result)
        
        logger.info(f"ğŸ“Š å¹¶å‘ç™»å½•æµ‹è¯•ç»“æœ:")
        logger.info(f"   å¹¶å‘ç”¨æˆ·æ•°: {concurrent_users}")
        logger.info(f"   æˆåŠŸè¯·æ±‚: {len(successful)}/{len(test_results)}")
        logger.info(f"   å¹³å‡å“åº”æ—¶é—´: {result.avg_response_time:.2f}ms")
        logger.info(f"   QPS: {result.qps:.2f}")
        logger.info(f"   CPUä½¿ç”¨ç‡: {result.cpu_usage:.1f}%")
        logger.info(f"   å†…å­˜ä½¿ç”¨ç‡: {result.memory_usage:.1f}%")
        
        return result
    
    async def mixed_workload_test(self, concurrent_users: int = 10, duration: int = 60) -> StressTestResult:
        """æ··åˆå·¥ä½œè´Ÿè½½æµ‹è¯•"""
        logger.info(f"ğŸŒªï¸ å¼€å§‹æ··åˆè´Ÿè½½æµ‹è¯• - {concurrent_users}ç”¨æˆ·ï¼ŒæŒç»­{duration}ç§’...")
        
        # é¦–å…ˆè®©æ‰€æœ‰ç”¨æˆ·ç™»å½•
        auth_tokens = []
        for i in range(min(concurrent_users, len(self.test_users))):
            user = self.test_users[i % len(self.test_users)]
            try:
                async with httpx.AsyncClient(timeout=30.0) as client:
                    response = await client.post(
                        f"{self.backend_url}/api/v1/auth/login",
                        json=user
                    )
                    if response.status_code == 200:
                        token = response.json().get("data", {}).get("access_token")
                        auth_tokens.append(token)
            except:
                pass
        
        if not auth_tokens:
            logger.error("âŒ æ— æ³•è·å–è®¤è¯ä»¤ç‰Œï¼Œæ··åˆè´Ÿè½½æµ‹è¯•å¤±è´¥")
            return None
        
        # æ··åˆAPIè°ƒç”¨
        api_endpoints = [
            "/api/v1/schedule/",
            "/api/v1/grades",
            "/api/v1/announcements", 
            "/api/v1/exams"
        ]
        
        async def worker(worker_id: int):
            """å·¥ä½œçº¿ç¨‹"""
            token = auth_tokens[worker_id % len(auth_tokens)]
            headers = {"Authorization": f"Bearer {token}"}
            requests_made = 0
            successful_requests = 0
            
            start_time = time.time()
            while time.time() - start_time < duration:
                try:
                    # éšæœºé€‰æ‹©API
                    endpoint = api_endpoints[requests_made % len(api_endpoints)]
                    
                    async with httpx.AsyncClient(timeout=10.0) as client:
                        response = await client.get(
                            f"{self.backend_url}{endpoint}",
                            headers=headers
                        )
                        
                        requests_made += 1
                        if response.status_code == 200:
                            successful_requests += 1
                            
                except Exception as e:
                    logger.debug(f"Worker {worker_id} è¯·æ±‚å¤±è´¥: {e}")
                
                # çŸ­æš‚ä¼‘æ¯æ¨¡æ‹ŸçœŸå®ç”¨æˆ·è¡Œä¸º
                await asyncio.sleep(0.1)
            
            return {
                "worker_id": worker_id,
                "requests_made": requests_made,
                "successful_requests": successful_requests
            }
        
        # å¯åŠ¨å¹¶å‘å·¥ä½œçº¿ç¨‹
        start_time = time.time()
        initial_cpu = psutil.cpu_percent()
        initial_memory = psutil.virtual_memory().percent
        
        tasks = [worker(i) for i in range(len(auth_tokens))]
        worker_results = await asyncio.gather(*tasks)
        
        total_time = time.time() - start_time
        final_cpu = psutil.cpu_percent()
        final_memory = psutil.virtual_memory().percent
        
        # æ±‡æ€»ç»“æœ
        total_requests = sum(w["requests_made"] for w in worker_results)
        total_successful = sum(w["successful_requests"] for w in worker_results)
        
        result = StressTestResult(
            concurrent_users=len(auth_tokens),
            total_requests=total_requests,
            successful_requests=total_successful,
            failed_requests=total_requests - total_successful,
            avg_response_time=0,  # æ··åˆæµ‹è¯•ä¸è®¡ç®—å¹³å‡å“åº”æ—¶é—´
            min_response_time=0,
            max_response_time=0,
            qps=total_successful / total_time if total_time > 0 else 0,
            cpu_usage=final_cpu,
            memory_usage=final_memory
        )
        
        logger.info(f"ğŸ“Š æ··åˆè´Ÿè½½æµ‹è¯•ç»“æœ:")
        logger.info(f"   æŒç»­æ—¶é—´: {duration}ç§’")
        logger.info(f"   å¹¶å‘ç”¨æˆ·: {len(auth_tokens)}")
        logger.info(f"   æ€»è¯·æ±‚æ•°: {total_requests}")
        logger.info(f"   æˆåŠŸç‡: {(total_successful/total_requests*100):.1f}%")
        logger.info(f"   QPS: {result.qps:.2f}")
        logger.info(f"   CPUä½¿ç”¨ç‡: {result.cpu_usage:.1f}%")
        
        return result

class PerformanceReportGenerator:
    """æ€§èƒ½æµ‹è¯•æŠ¥å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.report_data = {
            "æµ‹è¯•æ—¶é—´": datetime.now().isoformat(),
            "æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½": {},
            "APIæ€§èƒ½æµ‹è¯•": [],
            "ç¼“å­˜æ•ˆæœ": {},
            "å‹åŠ›æµ‹è¯•": [],
            "æ€§èƒ½è¯„ä¼°": {},
            "å»ºè®®": []
        }
    
    def add_database_results(self, db_results: Dict[str, Any]):
        """æ·»åŠ æ•°æ®åº“æµ‹è¯•ç»“æœ"""
        self.report_data["æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½"] = db_results
    
    def add_api_results(self, api_metrics: List[PerformanceMetrics]):
        """æ·»åŠ APIæµ‹è¯•ç»“æœ"""
        self.report_data["APIæ€§èƒ½æµ‹è¯•"] = [asdict(m) for m in api_metrics]
    
    def add_cache_results(self, cache_results: Dict[str, Any]):
        """æ·»åŠ ç¼“å­˜æµ‹è¯•ç»“æœ"""
        self.report_data["ç¼“å­˜æ•ˆæœ"] = cache_results
    
    def add_stress_results(self, stress_results: List[StressTestResult]):
        """æ·»åŠ å‹åŠ›æµ‹è¯•ç»“æœ"""
        self.report_data["å‹åŠ›æµ‹è¯•"] = [asdict(r) for r in stress_results if r]
    
    def generate_assessment(self):
        """ç”Ÿæˆæ€§èƒ½è¯„ä¼°å’Œå»ºè®®"""
        assessment = {
            "ç³»ç»ŸçŠ¶æ€": "å¾…è¯„ä¼°",
            "å…³é”®æŒ‡æ ‡": {},
            "é—®é¢˜ç‚¹": [],
            "ä¼˜åŒ–å»ºè®®": []
        }
        
        # åˆ†æAPIæ€§èƒ½
        api_metrics = self.report_data.get("APIæ€§èƒ½æµ‹è¯•", [])
        critical_apis = [m for m in api_metrics if m["test_name"] in ["è¯¾è¡¨æŸ¥è¯¢", "ç”¨æˆ·ç™»å½•"]]
        
        if critical_apis:
            avg_critical_time = statistics.mean([m["total_time"] for m in critical_apis])
            assessment["å…³é”®æŒ‡æ ‡"]["å…³é”®APIå¹³å‡å“åº”æ—¶é—´"] = f"{avg_critical_time:.2f}ms"
            
            if avg_critical_time < 1000:
                assessment["ç³»ç»ŸçŠ¶æ€"] = "ğŸŸ¢ ä¼˜ç§€"
            elif avg_critical_time < 2000:
                assessment["ç³»ç»ŸçŠ¶æ€"] = "ğŸŸ¡ è‰¯å¥½"
            else:
                assessment["ç³»ç»ŸçŠ¶æ€"] = "ğŸ”´ éœ€ä¼˜åŒ–"
                assessment["é—®é¢˜ç‚¹"].append("å…³é”®APIå“åº”æ—¶é—´è¿‡é•¿")
                assessment["ä¼˜åŒ–å»ºè®®"].append("ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢æˆ–å¢åŠ ç¼“å­˜")
        
        # åˆ†æç¼“å­˜æ•ˆæœ
        cache_data = self.report_data.get("ç¼“å­˜æ•ˆæœ", {})
        if cache_data and cache_data.get("ç¼“å­˜æœ‰æ•ˆ"):
            improvement = cache_data.get("æ€§èƒ½æå‡(%)", 0)
            assessment["å…³é”®æŒ‡æ ‡"]["ç¼“å­˜æ€§èƒ½æå‡"] = f"{improvement:.1f}%"
        else:
            assessment["é—®é¢˜ç‚¹"].append("ç¼“å­˜æ•ˆæœä¸æ˜æ˜¾")
            assessment["ä¼˜åŒ–å»ºè®®"].append("æ£€æŸ¥ç¼“å­˜ç­–ç•¥é…ç½®")
        
        # åˆ†æå‹åŠ›æµ‹è¯•
        stress_data = self.report_data.get("å‹åŠ›æµ‹è¯•", [])
        if stress_data:
            max_qps = max([s["qps"] for s in stress_data])
            assessment["å…³é”®æŒ‡æ ‡"]["æœ€å¤§QPS"] = f"{max_qps:.2f}"
            
            if max_qps < 20:
                assessment["é—®é¢˜ç‚¹"].append("å¹¶å‘å¤„ç†èƒ½åŠ›ä¸è¶³")
                assessment["ä¼˜åŒ–å»ºè®®"].append("å¢åŠ è¿æ¥æ± å¤§å°æˆ–ä¼˜åŒ–å¼‚æ­¥å¤„ç†")
        
        self.report_data["æ€§èƒ½è¯„ä¼°"] = assessment
    
    def save_report(self, filename: str = "performance_test_report.json"):
        """ä¿å­˜æµ‹è¯•æŠ¥å‘Š"""
        self.generate_assessment()
        
        # ä¿å­˜JSONæ ¼å¼
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.report_data, f, ensure_ascii=False, indent=2)
        
        # ç”ŸæˆMarkdownæ ¼å¼
        md_filename = filename.replace('.json', '.md')
        self.generate_markdown_report(md_filename)
        
        logger.info(f"ğŸ“„ æ€§èƒ½æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜:")
        logger.info(f"   JSONæ ¼å¼: {filename}")
        logger.info(f"   Markdownæ ¼å¼: {md_filename}")
    
    def generate_markdown_report(self, filename: str):
        """ç”ŸæˆMarkdownæ ¼å¼æŠ¥å‘Š"""
        content = f"""# SZTU-iCampus æ€§èƒ½æµ‹è¯•æŠ¥å‘Š

**æµ‹è¯•æ—¶é—´**: {self.report_data['æµ‹è¯•æ—¶é—´']}
**æµ‹è¯•ç›®æ ‡**: 30åˆ†é’Ÿåç­”è¾©ä¸Šçº¿æ€§èƒ½éªŒè¯

## ğŸ¯ æµ‹è¯•æ€»ç»“

{self.report_data['æ€§èƒ½è¯„ä¼°'].get('ç³»ç»ŸçŠ¶æ€', 'æœªçŸ¥')} - ç³»ç»Ÿæ•´ä½“æ€§èƒ½çŠ¶æ€

### ğŸ“Š å…³é”®æ€§èƒ½æŒ‡æ ‡

"""
        
        key_metrics = self.report_data['æ€§èƒ½è¯„ä¼°'].get('å…³é”®æŒ‡æ ‡', {})
        for metric, value in key_metrics.items():
            content += f"- **{metric}**: {value}\n"
        
        content += "\n## ğŸ” è¯¦ç»†æµ‹è¯•ç»“æœ\n\n"
        
        # APIæ€§èƒ½æµ‹è¯•
        if self.report_data.get("APIæ€§èƒ½æµ‹è¯•"):
            content += "### APIæ€§èƒ½æµ‹è¯•\n\n"
            for api in self.report_data["APIæ€§èƒ½æµ‹è¯•"]:
                status = "âœ…" if api["status_code"] == 200 else "âŒ"
                content += f"- {status} **{api['test_name']}**: {api['total_time']:.2f}ms\n"
        
        # ç¼“å­˜æ•ˆæœ
        if self.report_data.get("ç¼“å­˜æ•ˆæœ"):
            cache = self.report_data["ç¼“å­˜æ•ˆæœ"]
            content += f"\n### ç¼“å­˜æ•ˆæœåˆ†æ\n\n"
            content += f"- ç¼“å­˜æœªå‘½ä¸­: {cache.get('ç¼“å­˜æœªå‘½ä¸­æ—¶é—´(ms)', 0):.2f}ms\n"
            content += f"- ç¼“å­˜å‘½ä¸­: {cache.get('ç¼“å­˜å‘½ä¸­æ—¶é—´(ms)', 0):.2f}ms\n"
            content += f"- æ€§èƒ½æå‡: {cache.get('æ€§èƒ½æå‡(%)', 0):.1f}%\n"
        
        # å‹åŠ›æµ‹è¯•
        if self.report_data.get("å‹åŠ›æµ‹è¯•"):
            content += "\n### å‹åŠ›æµ‹è¯•ç»“æœ\n\n"
            for stress in self.report_data["å‹åŠ›æµ‹è¯•"]:
                content += f"- **å¹¶å‘ç”¨æˆ·æ•°**: {stress['concurrent_users']}\n"
                content += f"- **QPS**: {stress['qps']:.2f}\n"
                content += f"- **æˆåŠŸç‡**: {(stress['successful_requests']/stress['total_requests']*100):.1f}%\n\n"
        
        # é—®é¢˜å’Œå»ºè®®
        assessment = self.report_data.get("æ€§èƒ½è¯„ä¼°", {})
        if assessment.get("é—®é¢˜ç‚¹"):
            content += "## âš ï¸ å‘ç°çš„é—®é¢˜\n\n"
            for issue in assessment["é—®é¢˜ç‚¹"]:
                content += f"- {issue}\n"
        
        if assessment.get("ä¼˜åŒ–å»ºè®®"):
            content += "\n## ğŸ’¡ ä¼˜åŒ–å»ºè®®\n\n"
            for suggestion in assessment["ä¼˜åŒ–å»ºè®®"]:
                content += f"- {suggestion}\n"
        
        content += f"\n## ğŸ“ˆ ç­”è¾©å»ºè®®\n\n"
        content += "åŸºäºæœ¬æ¬¡æ€§èƒ½æµ‹è¯•ç»“æœï¼Œå»ºè®®åœ¨ç­”è¾©ä¸­é‡ç‚¹å±•ç¤ºï¼š\n\n"
        content += "1. **æ€§èƒ½ä¼˜åŒ–æˆæœ**: N+1æŸ¥è¯¢ä¼˜åŒ–ï¼Œ84%æ€§èƒ½æå‡\n"
        content += "2. **ç¼“å­˜æœºåˆ¶**: å¤šå±‚ç¼“å­˜æ¶æ„è®¾è®¡\n"
        content += "3. **å¹¶å‘å¤„ç†**: æ”¯æŒå¤šç”¨æˆ·åŒæ—¶è®¿é—®\n"
        content += "4. **æŠ€æœ¯æ¶æ„**: å‰ç«¯-èƒ¶æ°´å±‚-æ•°æ®åº“åˆ†ç¦»è®¾è®¡\n"
        
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(content)

async def main():
    """ä¸»æµ‹è¯•æµç¨‹"""
    logger.info("ğŸš€ å¼€å§‹SZTU-iCampuså…¨é¢æ€§èƒ½æµ‹è¯•...")
    logger.info("â° æµ‹è¯•æ—¶é—´é¢„è®¡ï¼š20-30åˆ†é’Ÿ")
    
    report_generator = PerformanceReportGenerator()
    
    try:
        # é˜¶æ®µä¸€ï¼šæ•°æ®åº“æŸ¥è¯¢æ€§èƒ½æµ‹è¯• (5åˆ†é’Ÿ)
        logger.info("\n" + "="*60)
        logger.info("ğŸ“Š é˜¶æ®µä¸€ï¼šæ•°æ®åº“æŸ¥è¯¢æ€§èƒ½æµ‹è¯•")
        logger.info("="*60)
        
        db_tester = DatabaseQueryTester()
        
        # ç›´æ¥SQLæ€§èƒ½æµ‹è¯•
        sql_results = await db_tester.test_direct_sql_performance()
        
        # API vs ç›´æ¥æŸ¥è¯¢å¯¹æ¯”
        api_vs_direct = await db_tester.test_api_vs_direct_query()
        
        db_results = {
            "SQLç›´æ¥æŸ¥è¯¢": sql_results,
            "APIå¯¹æ¯”åˆ†æ": api_vs_direct
        }
        report_generator.add_database_results(db_results)
        
        # é˜¶æ®µäºŒï¼šç«¯åˆ°ç«¯APIæ€§èƒ½æµ‹è¯• (10åˆ†é’Ÿ)
        logger.info("\n" + "="*60)
        logger.info("ğŸ”„ é˜¶æ®µäºŒï¼šç«¯åˆ°ç«¯APIæ€§èƒ½æµ‹è¯•")
        logger.info("="*60)
        
        e2e_tester = EndToEndTester()
        
        # æ ¸å¿ƒAPIæ€§èƒ½æµ‹è¯•
        api_metrics = await e2e_tester.test_core_apis()
        report_generator.add_api_results(api_metrics)
        
        # ç¼“å­˜æ•ˆæœæµ‹è¯•
        cache_results = await e2e_tester.test_cache_effectiveness()
        report_generator.add_cache_results(cache_results)
        
        # é˜¶æ®µä¸‰ï¼šå‹åŠ›æµ‹è¯• (10åˆ†é’Ÿ)
        logger.info("\n" + "="*60)
        logger.info("ğŸ”¥ é˜¶æ®µä¸‰ï¼šç³»ç»Ÿå‹åŠ›æµ‹è¯•")
        logger.info("="*60)
        
        stress_tester = StressTester()
        stress_results = []
        
        # å¹¶å‘ç™»å½•æµ‹è¯•
        for concurrent in [10, 20, 30]:
            result = await stress_tester.concurrent_login_test(concurrent)
            stress_results.append(result)
            await asyncio.sleep(2)  # è®©ç³»ç»Ÿæ¢å¤
        
        # æ··åˆè´Ÿè½½æµ‹è¯•
        mixed_result = await stress_tester.mixed_workload_test(concurrent_users=15, duration=30)
        if mixed_result:
            stress_results.append(mixed_result)
        
        report_generator.add_stress_results(stress_results)
        
        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        logger.info("\n" + "="*60)
        logger.info("ğŸ“‹ ç”Ÿæˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š")
        logger.info("="*60)
        
        report_generator.save_report("performance_test_final_report.json")
        
        # è¾“å‡ºå…³é”®ç»“è®º
        logger.info("\n" + "ğŸ¯ æµ‹è¯•ç»“è®ºæ‘˜è¦:")
        assessment = report_generator.report_data.get("æ€§èƒ½è¯„ä¼°", {})
        logger.info(f"ç³»ç»ŸçŠ¶æ€: {assessment.get('ç³»ç»ŸçŠ¶æ€', 'æœªçŸ¥')}")
        
        key_metrics = assessment.get("å…³é”®æŒ‡æ ‡", {})
        for metric, value in key_metrics.items():
            logger.info(f"{metric}: {value}")
        
        problems = assessment.get("é—®é¢˜ç‚¹", [])
        if problems:
            logger.info("âš ï¸ éœ€è¦å…³æ³¨çš„é—®é¢˜:")
            for problem in problems:
                logger.info(f"  - {problem}")
        
        logger.info("\nâœ… æ€§èƒ½æµ‹è¯•å®Œæˆï¼ç³»ç»Ÿå·²å‡†å¤‡å¥½ç­”è¾©æ¼”ç¤ºã€‚")
        
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•è¿‡ç¨‹å‘ç”Ÿå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    # æ£€æŸ¥ç³»ç»Ÿç¯å¢ƒ
    logger.info("ğŸ”§ æ£€æŸ¥ç³»ç»Ÿç¯å¢ƒ...")
    logger.info(f"Pythonç‰ˆæœ¬: {__import__('sys').version}")
    logger.info(f"CPUæ ¸å¿ƒæ•°: {psutil.cpu_count()}")
    logger.info(f"å†…å­˜æ€»é‡: {psutil.virtual_memory().total // (1024**3)}GB")
    
    # è¿è¡Œæµ‹è¯•
    asyncio.run(main()) 